{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['message', 'category']\n",
    "data = pd.read_csv(\"EnglishDataSet.csv\", usecols=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(message):\n",
    "    \n",
    "# remove punctuations\n",
    "    message = [char for char in message if char not in string.punctuation]\n",
    "# join the message after removing   \n",
    "    message = ''.join(message)\n",
    "    \n",
    "    return [word for word in message.split() if word.lower() not in stopwords.words('english')]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [Go, jurong, point, crazy, Available, bugis, n...\n",
       "1                          [Ok, lar, Joking, wif, u, oni]\n",
       "2       [Free, entry, 2, wkly, comp, win, FA, Cup, fin...\n",
       "3           [U, dun, say, early, hor, U, c, already, say]\n",
       "4       [Nah, dont, think, goes, usf, lives, around, t...\n",
       "                              ...                        \n",
       "5567    [2nd, time, tried, 2, contact, u, U, Â£750, Po...\n",
       "5568                   [Ã, b, going, esplanade, fr, home]\n",
       "5569                     [Pity, mood, Soany, suggestions]\n",
       "5570    [guy, bitching, acted, like, id, interested, b...\n",
       "5571                                   [Rofl, true, name]\n",
       "Name: message, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply the text processing to messages column\n",
    "data['message'].apply(text_process)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message category\n",
       "0  Go until jurong point, crazy.. Available only ...      ham\n",
       "1                      Ok lar... Joking wif u oni...      ham\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...     spam\n",
       "3  U dun say so early hor... U c already then say...      ham\n",
       "4  Nah I don't think he goes to usf, he lives aro...      ham"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorization and Term-Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a vectorizer\n",
    "vectorizer = CountVectorizer().fit(data['message'])\n",
    "vec_file = 'vectorizer'\n",
    "pickle.dump(vectorizer, open(vec_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1070)\t1\n",
      "  (0, 1304)\t1\n",
      "  (0, 1752)\t1\n",
      "  (0, 1754)\t1\n",
      "  (0, 2051)\t1\n",
      "  (0, 2330)\t1\n",
      "  (0, 3553)\t1\n",
      "  (0, 3597)\t1\n",
      "  (0, 3637)\t1\n",
      "  (0, 4091)\t1\n",
      "  (0, 4351)\t1\n",
      "  (0, 4477)\t1\n",
      "  (0, 5541)\t1\n",
      "  (0, 5924)\t1\n",
      "  (0, 7652)\t1\n",
      "  (0, 8037)\t1\n",
      "  (0, 8275)\t1\n",
      "  (0, 8498)\t1\n",
      "  (1, 4319)\t1\n",
      "  (1, 4513)\t1\n",
      "  (1, 5508)\t1\n",
      "  (1, 5537)\t1\n",
      "  (1, 8401)\t1\n",
      "  (2, 77)\t1\n",
      "  (2, 402)\t1\n",
      "  :\t:\n",
      "  (5570, 1781)\t1\n",
      "  (5570, 1789)\t1\n",
      "  (5570, 2595)\t1\n",
      "  (5570, 2895)\t1\n",
      "  (5570, 3311)\t1\n",
      "  (5570, 3361)\t1\n",
      "  (5570, 3473)\t1\n",
      "  (5570, 3690)\t1\n",
      "  (5570, 3784)\t1\n",
      "  (5570, 4091)\t1\n",
      "  (5570, 4165)\t1\n",
      "  (5570, 4222)\t1\n",
      "  (5570, 4617)\t1\n",
      "  (5570, 5338)\t1\n",
      "  (5570, 7046)\t1\n",
      "  (5570, 7056)\t1\n",
      "  (5570, 7634)\t1\n",
      "  (5570, 7763)\t1\n",
      "  (5570, 8072)\t1\n",
      "  (5570, 8321)\t1\n",
      "  (5571, 4229)\t2\n",
      "  (5571, 5248)\t1\n",
      "  (5571, 6510)\t1\n",
      "  (5571, 7763)\t1\n",
      "  (5571, 7892)\t1\n"
     ]
    }
   ],
   "source": [
    "# message4 = data['message'][3]\n",
    "# message4 = vectorizer.transform([message4])\n",
    "# print(message4)\n",
    "\n",
    "message = vectorizer.transform(data['message'])\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the messages into bag of words using vectorizer\n",
    "bag_of_words_messages = vectorizer.transform(data['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Sparse Matrix:  (5572, 8666)\n",
      "Amount of Non-Zero occurences:  73912\n"
     ]
    }
   ],
   "source": [
    "print('Shape of Sparse Matrix: ', bag_of_words_messages.shape)\n",
    "print('Amount of Non-Zero occurences: ', bag_of_words_messages.nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF for weights and normalization of vectors\n",
    "tf_transformer = TfidfTransformer().fit(bag_of_words_messages)\n",
    "vec_file = 'transformer'\n",
    "pickle.dump(tf_transformer, open(vec_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8498)\t0.22080132794235655\n",
      "  (0, 8275)\t0.18238655630689804\n",
      "  (0, 8037)\t0.22998520738984352\n",
      "  (0, 7652)\t0.15566431601878158\n",
      "  (0, 5924)\t0.2553151503985779\n",
      "  (0, 5541)\t0.15618023117358304\n",
      "  (0, 4477)\t0.2757654045621182\n",
      "  (0, 4351)\t0.3264252905795869\n",
      "  (0, 4091)\t0.10720385321563428\n",
      "  (0, 3637)\t0.1803175103691124\n",
      "  (0, 3597)\t0.15318864840197105\n",
      "  (0, 3553)\t0.1481298737377147\n",
      "  (0, 2330)\t0.25279391746019725\n",
      "  (0, 2051)\t0.2757654045621182\n",
      "  (0, 1754)\t0.2757654045621182\n",
      "  (0, 1752)\t0.3116082237740733\n",
      "  (0, 1304)\t0.24415547176756056\n",
      "  (0, 1070)\t0.3264252905795869\n",
      "  (1, 8401)\t0.4316010362639011\n",
      "  (1, 5537)\t0.5465881710238072\n",
      "  (1, 5508)\t0.27211951321382544\n",
      "  (1, 4513)\t0.4082988561907181\n",
      "  (1, 4319)\t0.5236458071582338\n",
      "  (2, 8456)\t0.18669123587240305\n",
      "  (2, 8414)\t0.14511814920515034\n",
      "  :\t:\n",
      "  (5570, 7056)\t0.20534071141898738\n",
      "  (5570, 7046)\t0.18426479853595398\n",
      "  (5570, 5338)\t0.21003407910338884\n",
      "  (5570, 4617)\t0.15965284335787472\n",
      "  (5570, 4222)\t0.12258970642239425\n",
      "  (5570, 4165)\t0.2829162285990171\n",
      "  (5570, 4091)\t0.11172759968405094\n",
      "  (5570, 3784)\t0.17077601391262975\n",
      "  (5570, 3690)\t0.24251246738414114\n",
      "  (5570, 3473)\t0.27527359955164493\n",
      "  (5570, 3361)\t0.15948689352397216\n",
      "  (5570, 3311)\t0.12171985462687529\n",
      "  (5570, 2895)\t0.24400620497703476\n",
      "  (5570, 2595)\t0.1845835068947418\n",
      "  (5570, 1789)\t0.2829162285990171\n",
      "  (5570, 1781)\t0.13664357413429448\n",
      "  (5570, 1547)\t0.34019965792896817\n",
      "  (5570, 1439)\t0.1429563528214243\n",
      "  (5570, 1085)\t0.11225095544329755\n",
      "  (5570, 904)\t0.3247573463060046\n",
      "  (5571, 7892)\t0.42752913176432156\n",
      "  (5571, 7763)\t0.14849350328973984\n",
      "  (5571, 6510)\t0.5565029307246045\n",
      "  (5571, 5248)\t0.39009002726386227\n",
      "  (5571, 4229)\t0.5773238083586979\n"
     ]
    }
   ],
   "source": [
    "tf4 = tf_transformer.transform(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_messages = tf_transformer.transform(bag_of_words_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 8666)\n"
     ]
    }
   ],
   "source": [
    "print(tf_idf_messages.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a training and testing set \n",
    "msg_train, msg_test, cat_train, cat_test = train_test_split(tf_idf_messages, data['category'], test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting into Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classify = MultinomialNB().fit(msg_train,cat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(msg_test)\n",
    "\n",
    "# nb_classify.predict(msg_test)\n",
    "\n",
    "predictionsNB = nb_classify.predict(msg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       1.00      0.96      0.98      1527\n",
      "        spam       0.70      1.00      0.82       145\n",
      "\n",
      "    accuracy                           0.96      1672\n",
      "   macro avg       0.85      0.98      0.90      1672\n",
      "weighted avg       0.97      0.96      0.97      1672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predictionsNB, cat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('naive_bayes','wb') as f:\n",
    "    pickle.dump(nb_classify,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting into Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_classify = LogisticRegression().fit(msg_train,cat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsLR = lr_classify.predict(msg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       1.00      0.94      0.97      1554\n",
      "        spam       0.54      0.99      0.70       118\n",
      "\n",
      "    accuracy                           0.94      1672\n",
      "   macro avg       0.77      0.96      0.83      1672\n",
      "weighted avg       0.97      0.94      0.95      1672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predictionsLR, cat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('logistic_regression','wb') as f:\n",
    "    pickle.dump(lr_classify,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting into Decision Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_classify = DecisionTreeClassifier().fit(msg_train,cat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsDT = dt_classify.predict(msg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      0.97      0.98      1470\n",
      "        spam       0.82      0.88      0.84       202\n",
      "\n",
      "    accuracy                           0.96      1672\n",
      "   macro avg       0.90      0.92      0.91      1672\n",
      "weighted avg       0.96      0.96      0.96      1672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predictionsDT, cat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('decision_tree','wb') as f:\n",
    "    pickle.dump(dt_classify,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
